{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import latex\n",
    "import spacy\n",
    "import itertools\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import financial_data_api as fd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from matplotlib import rc\n",
    "import matplotlib\n",
    "pgf_with_rc_fonts = {\"pgf.texsystem\": \"pdflatex\"}\n",
    "matplotlib.rcParams.update(pgf_with_rc_fonts)\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "financial_data = fd.FinancialData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "to the definition of the dot product\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputfile_from_name(model_name, metric, parent_directory):\n",
    "    fn = model_name + '_' + metric.replace(' ', '_')\n",
    "    f = os.path.join(parent_directory, fn)\n",
    "    return f + '.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers_in_directory():\n",
    "    return ['AAPL', 'CSCO', 'GOOGL', 'INTC', 'MCD', 'NFLX', 'TRV', 'VZ', 'AXP', 'CVX', 'GS', 'JNJ', 'MMM', 'NKE', 'UNH', 'WBA', 'BA', 'DIS', 'HD', 'JPM', 'MRK', 'PFE', 'UTX', 'WMT', \n",
    "           'CAT', 'DOW', 'IBM', 'KO', 'MSFT', 'PG', 'V', 'XOM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_from_tickers(tickers):\n",
    "    root_data_dir = 'article_data'\n",
    "    data_directories = [os.path.join(root_data_dir, t) for t in tickers]\n",
    "    return data_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_for_tickers(tickers):\n",
    "    path_names = get_paths_from_tickers(tickers)\n",
    "    articles = read_articles_from_disk(path_names)\n",
    "    return set(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_articles_from_disk(data_directories):\n",
    "    counter = 0\n",
    "    articles = []\n",
    "    for directory in data_directories: \n",
    "        file_list = os.listdir(directory)\n",
    "        files_to_read = [os.path.join(directory, file) for file in file_list]\n",
    "        for file in files_to_read: \n",
    "            with open(file, 'r') as content_file:\n",
    "                content = content_file.read()\n",
    "                articles.append(content)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(articles):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    all_sentences = []\n",
    "    counter = 0\n",
    "    \n",
    "    for a in articles:\n",
    "        print('Counter at: ', counter)\n",
    "        doc = nlp(a)\n",
    "        for s in doc.sents:\n",
    "            all_sentences.append(s)\n",
    "        counter += 1\n",
    "    return all_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_financial_metric_dict(tickers, metric):\n",
    "    result_dict = {}\n",
    "    for t in tickers: \n",
    "        data = financial_data.get_quarterly_data(t)\n",
    "        if t not in result_dict:\n",
    "            if data:\n",
    "                result_dict[t] = float(data[metric])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_closest_neighbors(company_graph, node_source, num_neighbors):\n",
    "    neighbors = nx.shortest_path_length(company_graph, source=node_source, weight='weight')\n",
    "    nearest = []\n",
    "    counter = 0\n",
    "    print('==============')\n",
    "    print('Target Node is: ', node_source)\n",
    "\n",
    "    for n in neighbors:\n",
    "        print('Neighbor is: ', n)\n",
    "        nearest.append(n)\n",
    "        if counter == num_neighbors:\n",
    "            break\n",
    "        counter += 1\n",
    "    print('==============')\n",
    "    return nearest[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_set(article, tickers):\n",
    "    words_found = set([])\n",
    "    ticker_set = set(tickers)\n",
    "    article = article.split()\n",
    "    for word in article:\n",
    "        if word in ticker_set: \n",
    "            words_found.add(word)\n",
    "    return words_found\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_list():\n",
    "    companies = [s for s in pd.read_csv('constituents.csv', header=0)['Symbol']]\n",
    "    companies.remove('A')\n",
    "    companies.remove('T')\n",
    "    companies.remove('GOOG')\n",
    "    return ['MMM', 'AXP', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'KO', 'DOW', 'XOM', 'GS', 'HD', 'IBM', 'INTC', 'JNJ', 'JPM',\n",
    "'MCD', 'MRK', 'MSFT', 'NKE', 'PFE', 'PG', 'TRV', 'UNH', 'UTX', 'VZ', 'V', 'WMT', 'WBA', 'DIS', 'GOOGL', 'NFLX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurrence_dict(articles):\n",
    "    counts_dict = {}\n",
    "    article_counter = 0\n",
    "    sp_500_list  = get_sp500_list()\n",
    "    for a in all_articles:\n",
    "        company_set = get_company_set(a, sp_500_list)\n",
    "        pairs = itertools.combinations(company_set, 2)\n",
    "        pairs = set([tuple(sorted(list(p))) for p in pairs])\n",
    "        for p in pairs: \n",
    "            if p not in counts_dict:\n",
    "                counts_dict[p] = 0\n",
    "            counts_dict[p] += 1\n",
    "    return counts_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_model(sentences, max_epochs, vec_size, alpha, name):\n",
    "    data = [str(s) for s in sentences]\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    \n",
    "    \n",
    "    #max_epochs = 100\n",
    "    #vec_size = 20\n",
    "    #alpha = 0.025\n",
    "\n",
    "    model = Doc2Vec(size=vec_size,\n",
    "                    alpha=alpha, \n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=1,\n",
    "                    dm =1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=model.iter)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    model.save(name)\n",
    "    print(\"Model Saved: \", name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_embeddings(sentences, inspect_set, model):\n",
    "    vec_size = len(model.docvecs['1'])\n",
    "    embedding_dict = {t:{'embedding_sum': np.zeros(vec_size), 'count':0} for t in inspect_set}\n",
    "    for s in sentences: \n",
    "        s_str = str(s).split()\n",
    "        model.random.seed(0)\n",
    "        s_str_embedding = model.infer_vector(word_tokenize(str(s).lower()))\n",
    "        for key in embedding_dict:\n",
    "            if key in s_str:\n",
    "                embedding_dict[key]['embedding_sum'] += s_str_embedding\n",
    "                embedding_dict[key]['count'] += 1\n",
    "    result_dict = {}\n",
    "    for k, v in embedding_dict.items():\n",
    "        result_dict[k] = v['embedding_sum']\n",
    "        if v['count'] > 0: \n",
    "            result_dict[k] /= v['count']\n",
    "    return result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc2vec_graph(name, f_metric, sentences):\n",
    "    model= Doc2Vec.load(\"d2v.model\")\n",
    "    inspect_set = get_sp500_list()\n",
    "    #embedding_vectors = get_average_embeddings(sentences, inspect_set, model)\n",
    "    pairs = itertools.combinations(inspect_set, 2)\n",
    "    ranked_scores = []\n",
    "\n",
    "    for p in pairs:\n",
    "        company_A = p[0]\n",
    "        company_B = p[1]\n",
    "        \n",
    "        model.random.seed(0)\n",
    "        vec_A = model.infer_vector([company_A])\n",
    "        model.random.seed(0)\n",
    "        vec_B = model.infer_vector([company_B])\n",
    "        \n",
    "        #vec_A = embedding_vectors[company_A].reshape(1, -1)\n",
    "        #vec_B = embedding_vectors[company_B].reshape(1, -1)\n",
    "        vec_A = vec_A.reshape(1, -1)\n",
    "        vec_B = vec_B.reshape(1, -1)\n",
    "\n",
    "        #cosine_distance = scipy.spatial.distance.cosine(vec_A, vec_B)\n",
    "        cosine_distance = 1 - cosine_similarity(vec_A, vec_B)[0][0]\n",
    "        ranked_scores.append((company_A, company_B, cosine_distance))\n",
    "    embedding_graph = nx.Graph()\n",
    "    embedding_graph.add_nodes_from(inspect_set)\n",
    "\n",
    "    embedding_graph.nodes()\n",
    "    \n",
    "    for rs in ranked_scores: \n",
    "        company_A = rs[0]\n",
    "        company_B = rs[1]\n",
    "        distance = rs[2]\n",
    "        print(company_A, company_B, distance)\n",
    "        embedding_graph.add_edge(company_A, company_B, weight=distance)\n",
    "    \n",
    "    k = 3\n",
    "    ebit_data = get_financial_metric_dict(inspect_set, f_metric)\n",
    "    results = []\n",
    "    n_dict = {}\n",
    "    for node in embedding_graph.nodes():\n",
    "        neighbors = get_k_closest_neighbors(embedding_graph, node, k)\n",
    "        neighbor_stats = []\n",
    "        n_dict[node] = neighbors\n",
    "\n",
    "        for n in neighbors: \n",
    "            if n in ebit_data:\n",
    "                neighbor_stats.append(ebit_data[n])\n",
    "        deviation = np.std(neighbor_stats)\n",
    "        results.append((node, deviation))\n",
    "    save_plots_for_results(results, name, f_metric)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cooccurrence_graph(all_articles, name, f_metric):\n",
    "    sp_500_list = get_sp500_list()\n",
    "    co_occurrence_dict = get_occurrence_dict(all_articles)\n",
    "    node_set = set([])\n",
    "    for pair in co_occurrence_dict:\n",
    "        node_set.add(pair[0])\n",
    "        node_set.add(pair[1])\n",
    "\n",
    "    co_occurrence_graph = nx.Graph()\n",
    "    co_occurrence_graph.add_nodes_from(list(node_set))\n",
    "\n",
    "    max_score = max([v for v in co_occurrence_dict.values()])\n",
    "\n",
    "    for pair, score in co_occurrence_dict.items():\n",
    "        company_A = pair[0]\n",
    "        company_B = pair[1]\n",
    "        co_occurrence_graph.add_edge(company_A, company_B, weight=(1 - (score/max_score)))\n",
    "    k = 3  \n",
    "    ebit_data = get_financial_metric_dict(node_set, f_metric)\n",
    "    results_baseline = []\n",
    "    n_dict = {}\n",
    "    for node in co_occurrence_graph.nodes():\n",
    "        neighbors = get_k_closest_neighbors(co_occurrence_graph, node, k)\n",
    "        neighbor_stats = []\n",
    "        n_dict[node] = neighbors\n",
    "        for n in neighbors: \n",
    "            if n in ebit_data:\n",
    "                neighbor_stats.append(ebit_data[n])\n",
    "        deviation = np.std(neighbor_stats)\n",
    "        results_baseline.append((node, deviation))\n",
    "    save_plots_for_results(results_baseline, name, f_metric)\n",
    "    return results_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots_for_results(results, output_file, metric):\n",
    "    top = sorted(results, key=lambda x:x[1])\n",
    "\n",
    "    labels, ys = zip(*top)\n",
    "    xs = np.arange(len(labels)) \n",
    "    width = 1\n",
    "    #plt.rc('text', usetex=True)\n",
    "\n",
    "    plt.bar(xs, ys, width, align='center')\n",
    "\n",
    "    plt.xticks(xs, labels, rotation='vertical') #Replace default x-ticks with xs, then replace xs with labels\n",
    "    #plt.yticks(ys)\n",
    "    plt.xlabel('Company')\n",
    "    plt.ylabel('Standard Deviation')\n",
    "    plt.title('Neighborhood STD for ' + metric)\n",
    "\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers_in_directory()\n",
    "all_articles = get_articles_for_tickers(tickers)\n",
    "#all_sentences = get_all_sentences(all_articles)\n",
    "all_sentences = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = [\n",
    "            'Gross Profit Growth', \n",
    "            'EBIT Growth', \n",
    "            'Operating Income Growth', \n",
    "            'Net Income Growth', \n",
    "            'EPS Growth', \n",
    "            'EPS Diluted Growth',\n",
    "            'Weighted Average Shares Growth',\n",
    "            'Dividends per Share Growth',\n",
    "            'Operating Cash Flow growth',\n",
    "            'Free Cash Flow growth',\n",
    "            'Receivables growth',\n",
    "            'Inventory Growth',\n",
    "            'Asset Growth',\n",
    "            'Book Value per Share Growth',\n",
    "            'Debt Growth',\n",
    "            'R&D Expense Growth',\n",
    "            'SG&A Expenses Growth'\n",
    "]\n",
    "for m in metrics_list:\n",
    "    results = get_doc2vec_graph(get_outputfile_from_name('doc2vec', m, 'output_naive'), m, all_sentences)\n",
    "    results_baseline = get_cooccurrence_graph(all_articles, get_outputfile_from_name('baseline', m, 'output_naive'), m)\n",
    "\n",
    "    improved_keys = {r[0]:r[1] for r in results}\n",
    "    baseline_keys = {r[0]:r[1] for r in results_baseline}\n",
    "    \n",
    "    difference_dict = {}\n",
    "    for key in improved_keys: \n",
    "        if key in baseline_keys: \n",
    "            difference_dict[key] = improved_keys[key] - baseline_keys[key]\n",
    "    counter = 0\n",
    "    for k, v in difference_dict.items():\n",
    "        if v <= 0: \n",
    "            counter += 1\n",
    "    print(m, counter)\n",
    "    \n",
    "    difference_results = [(k, v) for k, v in difference_dict.items()]\n",
    "    save_plots_for_results(difference_results, get_outputfile_from_name('difference', m, 'output_naive'), m)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
