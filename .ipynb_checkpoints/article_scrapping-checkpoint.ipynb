{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_urls(links_site):\n",
    "    '''scrape the html of the site'''\n",
    "    resp = requests.get(links_site)\n",
    " \n",
    "    if not resp.ok:\n",
    "        return None\n",
    " \n",
    "    html = resp.content\n",
    " \n",
    "    '''convert html to BeautifulSoup object'''\n",
    "    soup = BeautifulSoup(html , 'lxml')\n",
    " \n",
    "    '''get list of all links on webpage'''\n",
    "    links = soup.find_all('a')\n",
    " \n",
    "    urls = [link.get('href') for link in links]\n",
    "    urls = [url for url in urls if url is not None]\n",
    " \n",
    "    '''Filter the list of urls to just the news articles'''\n",
    "    news_urls = [url for url in urls if '/article/' in url]\n",
    " \n",
    "    return news_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_text(news_url):\n",
    " \n",
    "    news_html = requests.get(news_url).content\n",
    " \n",
    "    '''convert html to BeautifulSoup object'''\n",
    "    news_soup = BeautifulSoup(news_html , 'lxml')\n",
    " \n",
    "    paragraphs = [par.text for par in news_soup.find_all('p')]\n",
    "    news_text = '\\n'.join(paragraphs)\n",
    " \n",
    "    return news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_articles(ticker , upper_page_limit = 5):\n",
    " \n",
    "    landing_site = 'http://www.nasdaq.com/symbol/' + ticker + '/news-headlines'\n",
    " \n",
    "    all_news_urls = get_news_urls(landing_site)\n",
    " \n",
    "    current_urls_list = all_news_urls.copy()\n",
    " \n",
    "    index = 2\n",
    " \n",
    "    '''Loop through each sequential page, scraping the links from each'''\n",
    "    while (current_urls_list is not None) and (current_urls_list != []) and \\\n",
    "        (index <= upper_page_limit):\n",
    " \n",
    "        '''Construct URL for page in loop based off index'''\n",
    "        current_site = landing_site + '?page=' + str(index)\n",
    "        current_urls_list = get_news_urls(current_site)\n",
    " \n",
    "        '''Append current webpage's list of urls to all_news_urls'''\n",
    "        all_news_urls = all_news_urls + current_urls_list\n",
    " \n",
    "        index = index + 1\n",
    " \n",
    "    all_news_urls = list(set(all_news_urls))\n",
    " \n",
    "    '''Now, we have a list of urls, we need to actually scrape the text'''\n",
    "    all_articles = [scrape_news_text(news_url) for news_url in all_news_urls]\n",
    " \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_files(articles, ticker):\n",
    "    article_counter = 0\n",
    "    for article in articles: \n",
    "        article_file = ticker + str(article_counter) + '.txt'\n",
    "        file_name = os.path.join('article_data', article_file)\n",
    "\n",
    "        f = open(file_name, 'a')\n",
    "        f.write(article)\n",
    "        f.close()\n",
    "        article_counter += 1\n",
    "        print(\"Processed Article Number: \", article_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_articles(articles):\n",
    "    ad_string = \"Enter up to 25 symbols separated by commas or spaces in the text box\"\n",
    "    intro = \"Join the Nasdaq Community today and get free, instant access to portfolios, stock ratings, real-time alerts, and more!\"\n",
    "    processed_results = []\n",
    "    for a in articles: \n",
    "        ad_string_pos = a.find(ad_string)\n",
    "        intro_pos = a.find(intro)\n",
    "        start_index = intro_pos + len(intro)\n",
    "        end_index = ad_string_pos\n",
    "\n",
    "        processed_results.append(a[start_index:end_index])\n",
    "    \n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_articles(tickers, num_pages_to_read):\n",
    "    all_articles = []\n",
    "    for ticker in tickers:\n",
    "        raw_articles = scrape_all_articles(ticker, num_pages_to_read)\n",
    "        processed_articles = get_processed_articles(raw_articles)\n",
    "        all_articles += processed_articles \n",
    "    return all_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_set(article, ticker_dictionary, name_to_ticker):\n",
    "    words_found = set([])\n",
    "    ticker_set = set([])\n",
    "    for ticker, names in ticker_dictionary.items(): \n",
    "        ticker_set = ticker_set | names\n",
    "    article = article.split()\n",
    "    for word in article:\n",
    "        if word in ticker_set: \n",
    "            words_found.add(name_to_ticker[word])\n",
    "    return words_found \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graph(edge_graph, company_set):\n",
    "    pairs = itertools.combinations(company_set, 2)\n",
    "\n",
    "    for pair in pairs: \n",
    "        if pair in edge_graph or (pair[1], pair[0]) in edge_graph:\n",
    "            processed_pair = pair if pair in edge_graph else (pair[1], pair[0])\n",
    "            edge_graph[processed_pair] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_dictionary = {\n",
    "    'GOOGL':set(['Google', 'Alphabet', 'GOOGL', 'googl', 'google']),\n",
    "    'NFLX':set(['Netflix', 'NFLX', 'netflix', 'nflx']), \n",
    "    'MSFT':set(['MSFT', 'Microsoft', 'microsoft', 'MICROSOFT', 'msft']), \n",
    "    'AMZN':set(['AMZN', 'Amazon', 'amazon', 'amzn']), \n",
    "    'TSLA':set(['TSLA', 'TESLA', 'Tesla', 'tesla', 'tsla'])\n",
    "}\n",
    "name_to_ticker = {}\n",
    "for k, v, in ticker_dictionary.items():\n",
    "    for name in v: \n",
    "        name_to_ticker[name] = k \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('GOOGL', 'NFLX'): 47, ('GOOGL', 'MSFT'): 54, ('GOOGL', 'AMZN'): 69, ('GOOGL', 'TSLA'): 16, ('NFLX', 'MSFT'): 15, ('NFLX', 'AMZN'): 43, ('NFLX', 'TSLA'): 6, ('MSFT', 'AMZN'): 61, ('MSFT', 'TSLA'): 1, ('AMZN', 'TSLA'): 5}\n"
     ]
    }
   ],
   "source": [
    "import itertools \n",
    "edges = itertools.combinations(ticker_dictionary.keys(), 2)\n",
    "edge_graph = {e:0 for e in edges}\n",
    "num_pages_to_read = 10\n",
    "\n",
    "all_articles = get_all_articles(ticker_dictionary.keys(), num_pages_to_read)\n",
    "\n",
    "article_num = 0\n",
    "for article in all_articles: \n",
    "    article_num += 1\n",
    "    company_set = get_company_set(article, ticker_dictionary, name_to_ticker)\n",
    "    update_graph(edge_graph, company_set)\n",
    "\n",
    "    \n",
    "print(edge_graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
