{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_urls(links_site):\n",
    "    '''scrape the html of the site'''\n",
    "    resp = requests.get(links_site)\n",
    " \n",
    "    if not resp.ok:\n",
    "        return None\n",
    " \n",
    "    html = resp.content\n",
    " \n",
    "    '''convert html to BeautifulSoup object'''\n",
    "    soup = BeautifulSoup(html , 'lxml')\n",
    " \n",
    "    '''get list of all links on webpage'''\n",
    "    links = soup.find_all('a')\n",
    " \n",
    "    urls = [link.get('href') for link in links]\n",
    "    urls = [url for url in urls if url is not None]\n",
    " \n",
    "    '''Filter the list of urls to just the news articles'''\n",
    "    news_urls = [url for url in urls if '/article/' in url]\n",
    " \n",
    "    return news_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_text(news_url):\n",
    " \n",
    "    news_html = requests.get(news_url).content\n",
    " \n",
    "    '''convert html to BeautifulSoup object'''\n",
    "    news_soup = BeautifulSoup(news_html , 'lxml')\n",
    " \n",
    "    paragraphs = [par.text for par in news_soup.find_all('p')]\n",
    "    news_text = '\\n'.join(paragraphs)\n",
    "    print(\"Finished scrapping: \", news_url)\n",
    " \n",
    "    return news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_articles(ticker , upper_page_limit = 5):\n",
    " \n",
    "    landing_site = 'http://www.nasdaq.com/symbol/' + ticker + '/news-headlines'\n",
    " \n",
    "    all_news_urls = get_news_urls(landing_site)\n",
    " \n",
    "    current_urls_list = all_news_urls.copy()\n",
    " \n",
    "    index = 2\n",
    " \n",
    "    '''Loop through each sequential page, scraping the links from each'''\n",
    "    while (current_urls_list is not None) and (current_urls_list != []) and \\\n",
    "        (index <= upper_page_limit):\n",
    " \n",
    "        '''Construct URL for page in loop based off index'''\n",
    "        current_site = landing_site + '?page=' + str(index)\n",
    "        current_urls_list = get_news_urls(current_site)\n",
    " \n",
    "        '''Append current webpage's list of urls to all_news_urls'''\n",
    "        all_news_urls = all_news_urls + current_urls_list\n",
    " \n",
    "        index = index + 1\n",
    "        print(\"Done processing page: \", index)\n",
    " \n",
    "    all_news_urls = list(set(all_news_urls))\n",
    " \n",
    "    '''Now, we have a list of urls, we need to actually scrape the text'''\n",
    "    all_articles = [scrape_news_text(news_url) for news_url in all_news_urls]\n",
    " \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_files(articles, ticker):\n",
    "    article_counter = 0\n",
    "    for article in articles: \n",
    "        article_file = ticker + str(article_counter) + '.txt'\n",
    "        file_name = os.path.join('article_data', ticker, article_file)\n",
    "\n",
    "        f = open(file_name, 'w')\n",
    "        f.write(article)\n",
    "        f.close()\n",
    "        article_counter += 1\n",
    "        print(\"Processed Article Number: \", article_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_articles(articles):\n",
    "    ad_string = \"Enter up to 25 symbols separated by commas or spaces in the text box\"\n",
    "    intro = \"Join the Nasdaq Community today and get free, instant access to portfolios, stock ratings, real-time alerts, and more!\"\n",
    "    processed_results = []\n",
    "    for a in articles: \n",
    "        ad_string_pos = a.find(ad_string)\n",
    "        intro_pos = a.find(intro)\n",
    "        start_index = intro_pos + len(intro)\n",
    "        end_index = ad_string_pos\n",
    "\n",
    "        processed_results.append(a[start_index:end_index])\n",
    "    \n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_articles(tickers, num_pages_to_read):\n",
    "    all_articles = []\n",
    "    for ticker in tickers:\n",
    "        raw_articles = scrape_all_articles(ticker, num_pages_to_read)\n",
    "        processed_articles = get_processed_articles(raw_articles)\n",
    "        all_articles += processed_articles \n",
    "        print(\"Finished processing articles for: \", ticker)\n",
    "    return all_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_set(article, tickers):\n",
    "    words_found = set([])\n",
    "    ticker_set = set(tickers)\n",
    "    article = article.split()\n",
    "    for word in article:\n",
    "        if word in ticker_set: \n",
    "            words_found.add(word)\n",
    "    return words_found \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graph(edge_graph, company_set):\n",
    "    pairs = itertools.combinations(company_set, 2)\n",
    "\n",
    "    for pair in pairs: \n",
    "        if pair in edge_graph or (pair[1], pair[0]) in edge_graph:\n",
    "            processed_pair = pair if pair in edge_graph else (pair[1], pair[0])\n",
    "            edge_graph[processed_pair] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_articles_from_disk(data_directories):\n",
    "    counter = 0\n",
    "    articles = []\n",
    "    for directory in data_directories: \n",
    "        file_list = os.listdir(directory)\n",
    "        files_to_read = [os.path.join(directory, file) for file in file_list]\n",
    "        for file in files_to_read: \n",
    "            with open(file, 'r') as content_file:\n",
    "                content = content_file.read()\n",
    "                articles.append(content)\n",
    "    return articles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['WMT', 'WBA', 'VZ', 'V', 'UTX', 'PG', 'DIS']\n",
    "num_pages_to_read = 100\n",
    "\n",
    "for t in tickers: \n",
    "    ticker = [t]\n",
    "    all_articles = get_all_articles(ticker, num_pages_to_read)\n",
    "    save_articles_to_files(all_articles, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
