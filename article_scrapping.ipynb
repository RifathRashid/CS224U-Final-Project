{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_urls(links_site):\n",
    "    '''scrape the html of the site'''\n",
    "    resp = requests.get(links_site)\n",
    " \n",
    "    if not resp.ok:\n",
    "        return None\n",
    " \n",
    "    html = resp.content\n",
    " \n",
    "    '''convert html to BeautifulSoup object'''\n",
    "    soup = BeautifulSoup(html , 'lxml')\n",
    " \n",
    "    '''get list of all links on webpage'''\n",
    "    links = soup.find_all('a')\n",
    " \n",
    "    urls = [link.get('href') for link in links]\n",
    "    urls = [url for url in urls if url is not None]\n",
    " \n",
    "    '''Filter the list of urls to just the news articles'''\n",
    "    news_urls = [url for url in urls if '/article/' in url]\n",
    " \n",
    "    return news_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_text(news_url):\n",
    " \n",
    "    news_html = requests.get(news_url).content\n",
    " \n",
    "    '''convert html to BeautifulSoup object'''\n",
    "    news_soup = BeautifulSoup(news_html , 'lxml')\n",
    " \n",
    "    paragraphs = [par.text for par in news_soup.find_all('p')]\n",
    "    news_text = '\\n'.join(paragraphs)\n",
    "    print(\"Finished scrapping: \", news_url)\n",
    " \n",
    "    return news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_articles(ticker , upper_page_limit = 5):\n",
    " \n",
    "    landing_site = 'http://www.nasdaq.com/symbol/' + ticker + '/news-headlines'\n",
    " \n",
    "    all_news_urls = get_news_urls(landing_site)\n",
    " \n",
    "    current_urls_list = all_news_urls.copy()\n",
    " \n",
    "    index = 2\n",
    " \n",
    "    '''Loop through each sequential page, scraping the links from each'''\n",
    "    while (current_urls_list is not None) and (current_urls_list != []) and \\\n",
    "        (index <= upper_page_limit):\n",
    " \n",
    "        '''Construct URL for page in loop based off index'''\n",
    "        current_site = landing_site + '?page=' + str(index)\n",
    "        current_urls_list = get_news_urls(current_site)\n",
    " \n",
    "        '''Append current webpage's list of urls to all_news_urls'''\n",
    "        all_news_urls = all_news_urls + current_urls_list\n",
    " \n",
    "        index = index + 1\n",
    "        print(\"Done processing page: \", index)\n",
    " \n",
    "    all_news_urls = list(set(all_news_urls))\n",
    " \n",
    "    '''Now, we have a list of urls, we need to actually scrape the text'''\n",
    "    all_articles = [scrape_news_text(news_url) for news_url in all_news_urls]\n",
    " \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_files(articles, ticker):\n",
    "    article_counter = 0\n",
    "    for article in articles: \n",
    "        article_file = ticker + str(article_counter) + '.txt'\n",
    "        file_name = os.path.join('article_data', ticker, article_file)\n",
    "\n",
    "        f = open(file_name, 'w')\n",
    "        f.write(article)\n",
    "        f.close()\n",
    "        article_counter += 1\n",
    "        print(\"Processed Article Number: \", article_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_articles(articles):\n",
    "    ad_string = \"Enter up to 25 symbols separated by commas or spaces in the text box\"\n",
    "    intro = \"Join the Nasdaq Community today and get free, instant access to portfolios, stock ratings, real-time alerts, and more!\"\n",
    "    processed_results = []\n",
    "    for a in articles: \n",
    "        ad_string_pos = a.find(ad_string)\n",
    "        intro_pos = a.find(intro)\n",
    "        start_index = intro_pos + len(intro)\n",
    "        end_index = ad_string_pos\n",
    "\n",
    "        processed_results.append(a[start_index:end_index])\n",
    "    \n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_articles(tickers, num_pages_to_read):\n",
    "    all_articles = []\n",
    "    for ticker in tickers:\n",
    "        raw_articles = scrape_all_articles(ticker, num_pages_to_read)\n",
    "        processed_articles = get_processed_articles(raw_articles)\n",
    "        all_articles += processed_articles \n",
    "        print(\"Finished processing articles for: \", ticker)\n",
    "    return all_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_set(article, tickers):\n",
    "    words_found = set([])\n",
    "    ticker_set = set(tickers)\n",
    "    article = article.split()\n",
    "    for word in article:\n",
    "        if word in ticker_set: \n",
    "            words_found.add(word)\n",
    "    return words_found \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graph(edge_graph, company_set):\n",
    "    pairs = itertools.combinations(company_set, 2)\n",
    "\n",
    "    for pair in pairs: \n",
    "        if pair in edge_graph or (pair[1], pair[0]) in edge_graph:\n",
    "            processed_pair = pair if pair in edge_graph else (pair[1], pair[0])\n",
    "            edge_graph[processed_pair] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_articles_from_disk(data_directories):\n",
    "    counter = 0\n",
    "    articles = []\n",
    "    for directory in data_directories: \n",
    "        file_list = os.listdir(directory)\n",
    "        files_to_read = [os.path.join(directory, file) for file in file_list]\n",
    "        for file in files_to_read: \n",
    "            with open(file, 'r') as content_file:\n",
    "                content = content_file.read()\n",
    "                articles.append(content)\n",
    "    return articles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_names = ['AAPL', 'AXP', 'BA', 'CAT', 'CSCO', 'CVX', 'DOW', 'GOOGL', 'GS', 'HD', 'IBM', 'KO', 'MMM', 'MSFT', 'NFLX', 'XOM']\n",
    "root_data_dir = 'article_data'\n",
    "data_directories = [os.path.join(root_data_dir, d) for d in directory_names]\n",
    "\n",
    "all_articles = set(read_articles_from_disk(data_directories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_p_500 = [s for s in pd.read_csv('constituents.csv', header=0)['Symbol']]\n",
    "s_p_500.remove('A')\n",
    "s_p_500.remove('GOOG')\n",
    "s_p_500.remove('T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "import random \n",
    "edges = itertools.combinations(s_p_500, 2)\n",
    "edge_graph = {e:0 for e in edges}\n",
    "\n",
    "for article in all_articles:\n",
    "    company_set = get_company_set(article, s_p_500)\n",
    "    update_graph(edge_graph, company_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sorted([(v, k) for k, v in edge_graph.items()], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.cubical_graph()\n",
    "for edge in edge_graph:\n",
    "    node1 = edge[0]\n",
    "    node2 = edge[1]\n",
    "    \n",
    "    if node1 not in G.nodes():\n",
    "        G.add_node(node1)\n",
    "    \n",
    "    if node2 not in G.nodes():\n",
    "        G.add_node(node2)\n",
    "for edge, e_weight in edge_graph.items():\n",
    "    print(edge)\n",
    "    node1 = edge[0]\n",
    "    node2 = edge[1]\n",
    "    \n",
    "    G.add_edge(node1, node2, weight=e_weight)\n",
    "    \n",
    "nx.draw(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "comp_dict = {s:0 for s in s_p_500}\n",
    "\n",
    "article_num = 0\n",
    "for article in all_articles:\n",
    "    doc = nlp(article)\n",
    "    results = [(X.text, X.label_) for X in doc.ents]\n",
    "    seen_set = set([])\n",
    "    for r in results: \n",
    "        if r[1] == 'ORG':\n",
    "            seen_set.add(r[0])\n",
    "    print(seen_set)\n",
    "    print(comp_dict['AAPL'])\n",
    "    article_num += 1\n",
    "    print(\"Done with: \", article_num)\n",
    "comp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tickers = ['IBM']\n",
    "num_pages_to_read = 120\n",
    "all_articles = get_all_articles(tickers, num_pages_to_read)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ticker = tickers[0]\n",
    "save_articles_to_files(all_articles, ticker)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ticker_dictionary = {\n",
    "    'GOOGL':set(['Google', 'Alphabet', 'GOOGL', 'googl', 'google']),\n",
    "    'NFLX':set(['Netflix', 'NFLX', 'netflix', 'nflx']), \n",
    "    'MSFT':set(['MSFT', 'Microsoft', 'microsoft', 'MICROSOFT', 'msft']), \n",
    "    'AMZN':set(['AMZN', 'Amazon', 'amazon', 'amzn']), \n",
    "    'TSLA':set(['TSLA', 'TESLA', 'Tesla', 'tesla', 'tsla'])\n",
    "}\n",
    "\n",
    "\n",
    "dow_info = pd.read_html('https://finance.yahoo.com/quote/%5EDJI/components?p=%5EDJI')[0]['Symbol']\n",
    "dow_tickers = [t for t in dow_info]\n",
    "ticker_dictionary = {t:set([t]) for t in dow_tickers}\n",
    "\n",
    "name_to_ticker = {}\n",
    "for k, v, in ticker_dictionary.items():\n",
    "    for name in v: \n",
    "        name_to_ticker[name] = k \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import itertools \n",
    "edges = itertools.combinations(ticker_dictionary.keys(), 2)\n",
    "edge_graph = {e:0 for e in edges}\n",
    "num_pages_to_read = 100\n",
    "\n",
    "all_articles = get_all_articles(ticker_dictionary.keys(), num_pages_to_read)\n",
    "\n",
    "article_num = 0\n",
    "for article in all_articles: \n",
    "    article_num += 1\n",
    "    company_set = get_company_set(article, ticker_dictionary, name_to_ticker)\n",
    "    update_graph(edge_graph, company_set)\n",
    "\n",
    "    \n",
    "print(edge_graph)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "results = [(v, k) for k, v in edge_graph.items()]\n",
    "results = sorted(results, reverse=True)\n",
    "print(results)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
